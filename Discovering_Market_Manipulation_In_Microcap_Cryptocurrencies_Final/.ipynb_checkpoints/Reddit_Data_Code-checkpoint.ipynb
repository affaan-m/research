{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2de93be",
   "metadata": {},
   "source": [
    "    The tickers for the 49 cryptocurrencies were obtained through random sampling are shown below in the blocks code output. It's important to note that multiple cryptocurrencies may share the same ticker. However, CoinMarketCap's API assigns a unique ID to each currency, ensuring clarity in our analysis. Furthermore the 30 subreddits we scraped are below in both the code block as well as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12063ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XACT', 'IS', 'CHAD', 'DOGECEO', 'JADE', 'TITTER', 'ICE', 'CAH', 'RUNY', 'PRVC', 'TONI', 'BS', 'LAMBO', 'HBT', 'DOGECUBE', 'TSUBASAUT', 'LAI', '$L', 'ARCHI', 'FADER', 'HKTIGER', '$420CHAN', 'ZZZ', 'CGT', 'BLUE', 'PRNT', 'pogai', 'MAZI', 'REUNI', 'ZANGAI', 'SONIC', 'PER', '$TOAD', 'MUZZ', 'OT', 'GDX', 'pepecoin', 'CAWCEO', 'EVY', 'MILO', 'JIM', 'BLU', 'DATADOGE', 'NEMS', 'APED', 'SPC', 'MONKEYS', 'YOKEN', 'SWITCH']\n",
      "Finished scraping subreddit: SatoshiBets\n",
      "Finished scraping subreddit: Shitcoins\n",
      "Finished scraping subreddit: MoonBets\n",
      "Finished scraping subreddit: PancakeswapICO\n",
      "Finished scraping subreddit: AllCryptoBets\n",
      "Finished scraping subreddit: CryptoNews\n",
      "Finished scraping subreddit: airdrops\n",
      "Finished scraping subreddit: ethtrader\n",
      "Finished scraping subreddit: CryptoMoonShots\n",
      "Finished scraping subreddit: shitcoinmoonshots\n",
      "Finished scraping subreddit: Crypto_General\n",
      "Finished scraping subreddit: CryptoMars\n",
      "Finished scraping subreddit: crypto_currency\n",
      "Finished scraping subreddit: SatoshiStreetBets\n",
      "Finished scraping subreddit: BountyICO\n",
      "Finished scraping subreddit: CryptoCurrencyClassic\n",
      "Finished scraping subreddit: bscbombs\n",
      "Finished scraping subreddit: CryptoIDOS\n",
      "Finished scraping subreddit: SHIBADULTS\n",
      "Finished scraping subreddit: dogecoin\n",
      "Finished scraping subreddit: BSCMoonShots\n",
      "Finished scraping subreddit: BitcoinDiscussion\n",
      "Finished scraping subreddit: Shibainucoin\n",
      "Finished scraping subreddit: CryptoMoon\n",
      "Finished scraping subreddit: Shibu_Inu\n",
      "Finished scraping subreddit: ico\n",
      "Finished scraping subreddit: ICOAnalysis\n",
      "Finished scraping subreddit: CryptocurrencyICO\n",
      "Finished scraping subreddit: CryptoMarsShots\n",
      "Finished scraping subreddit: cryptostreetbets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import praw\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# read the list of microcaps from the excel file previously created when scraping the list of microcaps from coinmarketcap\n",
    "\n",
    "df_microcaps = pd.read_excel('microcaps.xlsx')\n",
    "microcaps = df_microcaps['symbol'].to_list()\n",
    "print(microcaps)\n",
    "\n",
    "# scrape the subreddits for the 49 microcaps we previously identified through random sampling using coinmarketcap\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"your client id\",\n",
    "    client_secret=\"your client secret\",\n",
    "    user_agent=\"Microcap Cryptos\",\n",
    ")\n",
    "\n",
    "# sample size of 30 subreddits to scrape all specified subreddits deal with microcaps\n",
    "\n",
    "subreddits = ['SatoshiBets', 'Shitcoins', 'MoonBets', 'PancakeswapICO', 'AllCryptoBets', 'CryptoNews', 'airdrops', 'ethtrader', 'CryptoMoonShots', 'shitcoinmoonshots', 'Crypto_General', 'CryptoMars', 'crypto_currency', 'SatoshiStreetBets', 'BountyICO', 'CryptoCurrencyClassic', 'bscbombs', 'CryptoIDOS', 'SHIBADULTS', 'dogecoin', 'BSCMoonShots', 'BitcoinDiscussion', 'Shibainucoin', 'CryptoMoon', 'Shibu_Inu', 'ico', 'ICOAnalysis', 'CryptocurrencyICO', 'CryptoMarsShots', 'cryptostreetbets']  # list of subreddits you're interested in\n",
    "keywords = microcaps  # list of keywords you're interested in\n",
    "\n",
    "# search all posts in all 30 subreddits for the 49 microcaps, collect the title, content, upvotes, subreddit, date, and author of each post, and save the results to a csv file.\n",
    "\n",
    "all_posts = []\n",
    "\n",
    "for subreddit_name in subreddits:\n",
    "    posts = []\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for keyword in keywords:\n",
    "        for submission in subreddit.search(keyword, time_filter='all'):\n",
    "            posts.append({\n",
    "                'Title': submission.title,\n",
    "                'Content': submission.selftext,\n",
    "                'Ups': submission.ups,\n",
    "                'Subreddit': str(subreddit),\n",
    "                'Date': datetime.datetime.fromtimestamp(submission.created),\n",
    "                'Author': str(submission.author),\n",
    "            })\n",
    "    df = pd.DataFrame(posts)\n",
    "    df.to_csv(f'{subreddit_name}_posts.csv', index=False)\n",
    "    all_posts.extend(posts)\n",
    "    print(f\"Finished scraping subreddit: {subreddit_name}\")\n",
    "    time.sleep(60)  # delay for 60 seconds\n",
    "\n",
    "    # save the results to a csv file\n",
    "\n",
    "df_all = pd.DataFrame(all_posts)\n",
    "df_all.to_csv('all_posts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9ae133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  The future of gaming is in the vibrant Neon Li...   \n",
      "1  $CUMINU is ready to compete with OnlyFans. Ond...   \n",
      "2  $CUMINU 5m mcap is ready to compete with OnlyF...   \n",
      "3  $CUMINU at $8.4M is about to launch a web3 18+...   \n",
      "4  üì£ Exciting News: Seneca Testnet and Devnet Pub...   \n",
      "5  Spodmoon is the First \"REAL\" Memecoin of the I...   \n",
      "6                               $SHENT IS SO EARLYüê≤üî•   \n",
      "7  $POOH is best recommended! | Launched stealth ...   \n",
      "8                        $NOOD is the next #100xgem!   \n",
      "9                        $NOOD is the next #100xgem!   \n",
      "\n",
      "                                             Content  Ups    Subreddit  \\\n",
      "0  https://preview.redd.it/tudc6doie88b1.png?widt...    2  SatoshiBets   \n",
      "1  The first milestone is having 500 creators and...   15  SatoshiBets   \n",
      "2  The first milestone is having 500 creators and...   10  SatoshiBets   \n",
      "3  $CUMINU has new and verified creators either f...   27  SatoshiBets   \n",
      "4  [Seneca Tech](https://seneca.tech/) is thrille...    2  SatoshiBets   \n",
      "5   **SpodMoon: Get Ready for the BSC Meme Season...   25  SatoshiBets   \n",
      "6  \\nWe have major plans.\\nThe dev to make it hap...    4  SatoshiBets   \n",
      "7                                                       1  SatoshiBets   \n",
      "8                                                       1  SatoshiBets   \n",
      "9                                                       1  SatoshiBets   \n",
      "\n",
      "                 Date              Author  \n",
      "0 2023-06-25 14:23:52  Academic-Fill-4258  \n",
      "1 2023-06-19 12:50:48        cutecoyote40  \n",
      "2 2023-06-20 11:32:52        cutecoyote40  \n",
      "3 2023-05-29 13:55:31        cutecoyote40  \n",
      "4 2023-06-25 19:10:21       ViralCrypto28  \n",
      "5 2023-05-17 12:36:45           Bigmanjr6  \n",
      "6 2023-06-06 12:24:55  Conscious_Egg_9389  \n",
      "7 2023-06-04 07:35:14       Long_Emu_3478  \n",
      "8 2023-06-29 22:35:39       Long_Emu_3478  \n",
      "9 2023-06-29 23:29:48            yan_yanM  \n"
     ]
    }
   ],
   "source": [
    "# read the csv file containing the scraped data to make sure it worked\n",
    "print(df_all.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66112d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title        24697\n",
       "Content      24697\n",
       "Ups          24697\n",
       "Subreddit    24697\n",
       "Date         24697\n",
       "Author       24697\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of posts scraped\n",
    "df_all.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
